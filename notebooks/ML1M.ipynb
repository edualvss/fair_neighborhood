{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from recpack.preprocessing.preprocessors import DataFramePreprocessor\n",
    "from recpack.preprocessing.filters import Deduplicate, MinRating, MinItemsPerUser\n",
    "from recpack.scenarios import WeakGeneralization\n",
    "\n",
    "from hyperopt import fmin, tpe, hp\n",
    "\n",
    "# helpers & metrics\n",
    "from src.helper_functions.data_formatting import *\n",
    "from src.helper_functions.metrics_accuracy import *\n",
    "from src.helper_functions.metrics_coverage import *\n",
    "from src.helper_functions.metrics_exposure import *\n",
    "\n",
    "# models\n",
    "from src.recommenders.ease import myEASE\n",
    "from src.recommenders.slim_bn import BNSLIM\n",
    "from src.recommenders.fslr import FSLR\n",
    "from src.recommenders.slim_bn_admm import BNSLIM_ADMM\n",
    "from src.recommenders.mf_fair import FairMF\n",
    "from src.recommenders.fda import FDA_bpr\n",
    "\n",
    "import json\n",
    "import re\n",
    "import itertools\n",
    "import time\n",
    "# import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load ratings.dat from ml-1m folder\n",
    "ratings = pd.read_csv(\"ml-1m/ratings.dat\", sep=\"::\", header=None, usecols=[0,1,2,3], names=[\"User_id\",\"Item_id\",\"Rating\",\"Timestamp\"], engine=\"python\")\n",
    "\n",
    "# load movies.dat from ml-1m folder\n",
    "movies = pd.read_csv(\"ml-1m/movies.dat\", sep=\"::\", header=None, usecols=[0,1,2], names=[\"Item_id\", \"Title\", \"Genre\"], encoding=\"latin-1\", engine=\"python\")\n",
    "movies[\"Genre\"] = movies[\"Genre\"].apply(lambda x: x.split(\"|\"))\n",
    "movies_items = movies\n",
    "movies = movies.explode(\"Genre\")\n",
    "\n",
    "# load users.dat from ml-1m folder\n",
    "users = pd.read_csv(\"ml-1m/users.dat\", sep=\"::\", header=None, usecols=[0,1], names=[\"User_id\", \"Gender\"], encoding=\"latin-1\", engine=\"python\")\n",
    "\n",
    "# replace \"M\" with 0 and \"F\" with 1 in the \"Gender\" column\n",
    "users[\"Gender\"] = users[\"Gender\"].replace({\"M\": 0, \"F\": 1})\n",
    "\n",
    "# join ratings on users with User_id\n",
    "ratings = pd.merge(ratings, users, on=\"User_id\", how=\"left\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ratings_pp = DataFramePreprocessor(\"Item_id\", \"User_id\",\"Timestamp\")\n",
    "\n",
    "# define filters\n",
    "deduplicate = Deduplicate(\"Item_id\", \"User_id\", \"Timestamp\")\n",
    "min_rating_filter = MinRating(4, \"Rating\")\n",
    "min_items_per_user_filter = MinItemsPerUser(10, \"Item_id\", \"User_id\")\n",
    "\n",
    "# add filters to pre-processor\n",
    "ratings_pp.add_filter(deduplicate)\n",
    "ratings_pp.add_filter(min_rating_filter)\n",
    "ratings_pp.add_filter(min_items_per_user_filter)\n",
    "\n",
    "# create interaction matrix object\n",
    "im = ratings_pp.process(ratings)\n",
    "\n",
    "# apply filters to ratings frame directly\n",
    "ratings = min_items_per_user_filter.apply(min_rating_filter.apply(deduplicate.apply(ratings)))\n",
    "\n",
    "movies = movies[movies[\"Item_id\"].isin(ratings[\"Item_id\"].unique())] # only keep items that are in the filtered ratings\n",
    "raw_genre_dict = dict(movies.groupby(\"Genre\")[\"Item_id\"].apply(lambda x: list(set(x))))\n",
    "\n",
    "# genre - inner iids dictionary\n",
    "inner_genre_dict = {\n",
    "    genre: get_inner_item_ids(ratings_pp, raw_iids)\n",
    "    for genre, raw_iids in raw_genre_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### UPDATE\n",
    "YEAR_MOVIE_TITLES_PATTERN = r'\\s*\\(\\d{4}\\)'\n",
    "original_movie_titles = movies.Title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### UPDATE\n",
    "original_to_updated_titles = {title : re.sub(YEAR_MOVIE_TITLES_PATTERN,'',title) for title in original_movie_titles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### UPDATE\n",
    "for title in original_to_updated_titles:\n",
    "    if \",\" in original_to_updated_titles[title]:\n",
    "        parts = original_to_updated_titles[title].split(\",\")\n",
    "        if len(parts) == 2:\n",
    "            if parts[1] == ' The':\n",
    "                original_to_updated_titles[title] = 'The ' + parts[0]\n",
    "            elif parts[1] == ' A':\n",
    "                original_to_updated_titles[title] = 'A ' + parts[0]\n",
    "        \n",
    "updated_to_original_titles = {original_to_updated_titles[original_title] : original_title for original_title in original_to_updated_titles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### UPDATE\n",
    "pd.DataFrame.sparse.from_spmatrix(im.binary_values).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# compute sparsity after filtering\n",
    "sparsity = 1 - im.density\n",
    "\n",
    "# calculate user interaction and item popularity ranges\n",
    "user_interactions = im.binary_values.sum(axis=1)\n",
    "item_popularities = im.binary_values.sum(axis=0)\n",
    "print(f\"User interaction ranges from {user_interactions.min()} to {user_interactions.max()}. Item popularity ranges from {item_popularities.min()} to {item_popularities.max()}.\")\n",
    "\n",
    "# get the raw ids of all users involved\n",
    "raw_uids = get_raw_user_ids(ratings_pp, im.active_users)\n",
    "\n",
    "# create uid - gender mapping df\n",
    "gender_mapping_df = ratings[ratings[\"User_id\"].isin(raw_uids)][[\"User_id\", \"Gender\"]].drop_duplicates()\n",
    "\n",
    "# get the raw/inner ids of all females involved\n",
    "raw_uids_f = gender_mapping_df.loc[gender_mapping_df[\"Gender\"] == 1, \"User_id\"].to_numpy()\n",
    "inner_uids_f = get_inner_user_ids(ratings_pp, raw_uids_f)\n",
    "\n",
    "# get the raw/inner ids of all males involved\n",
    "raw_uids_m = gender_mapping_df.loc[gender_mapping_df[\"Gender\"] == 0, \"User_id\"].to_numpy()\n",
    "inner_uids_m = get_inner_user_ids(ratings_pp, raw_uids_m)\n",
    "\n",
    "num_interactions_f, num_interactions_m = im.binary_values[inner_uids_f].sum(), im.binary_values[inner_uids_m].sum()\n",
    "\n",
    "# table stats\n",
    "statTable1 = PrettyTable([\"data set\",\"|U|\",\"|I|\",\"int(I)\",\"sparsity\"])\n",
    "statTable1.add_row([\"ML1M\", str(im.num_active_users), str(im.num_active_items), str(im.num_interactions), str(round(sparsity*100,2))])\n",
    "print(statTable1)\n",
    "\n",
    "statTable2 = PrettyTable([\"data set\",\"attribute\",\"|F|\",\"int(F)\",\"|M|\",\"int(M)\"])\n",
    "statTable2.add_row([\"ML1M\", \"gender\", str(len(raw_uids_f)), str(num_interactions_f), str(len(raw_uids_m)), str(num_interactions_m)])\n",
    "print(statTable2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### UPDATE\n",
    "pd.merge(ratings, movies, on=\"Item_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_merged = pd.merge(ratings, movies, on=\"Item_id\", how=\"left\")\n",
    "\n",
    "# Group by gender: 0 for Men and 1 for Women\n",
    "ratings_merged[\"Gender_Group\"] = (ratings_merged[\"Gender\"] == 1).astype(int)\n",
    "\n",
    "# Sum the number of interactions for each user within each genre and gender group\n",
    "user_genre_gender_sum = ratings_merged.groupby([\"User_id\", \"Genre\", \"Gender_Group\"]).size().reset_index(name=\"Interactions\")\n",
    "\n",
    "# Calculate the average number of interactions per user for each genre and gender group\n",
    "genre_gender_avg_interactions = user_genre_gender_sum.groupby([\"Genre\", \"Gender_Group\"])[\"Interactions\"].mean().reset_index()\n",
    "\n",
    "# Pivot the data for plotting\n",
    "genre_gender_avg_interactions_pivot = genre_gender_avg_interactions.pivot(index=\"Genre\", columns=\"Gender_Group\", values=\"Interactions\").fillna(0)\n",
    "\n",
    "# Sort genres by average interactions\n",
    "genre_gender_avg_interactions_pivot[\"Total\"] = (genre_gender_avg_interactions_pivot[0] + genre_gender_avg_interactions_pivot[1]) / 2\n",
    "genre_gender_avg_interactions_pivot_sorted = genre_gender_avg_interactions_pivot.sort_values(by=\"Total\", ascending=False).drop(columns=\"Total\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "genre_gender_avg_interactions_pivot_sorted.plot(kind=\"bar\", figsize=(15,10))\n",
    "\n",
    "plt.title(\"Average Number of Views per User by Genre and Gender\")\n",
    "plt.xlabel(\"Genre\")\n",
    "plt.ylabel(\"Average Number of Views per User\")\n",
    "plt.legend([\"Men\", \"Women\"], title=\"Gender Group\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define K for Top-K\n",
    "K = 20\n",
    "\n",
    "# Define alpha, the parameter that balances the importance of NDCG and Equity in the objective function.\n",
    "# Setting alpha = 0.5 gives equal weight to both metrics, aiming to balance relevance (NDCG) and fairness (Equity).\n",
    "# Adjusting alpha allows for prioritizing one metric over the other.\n",
    "# For instance, setting alpha closer to 1.0 would prioritize NDCG (accuracy), while setting it closer to 0.0 would prioritize Equity (fairness).\n",
    "alpha = 0.5\n",
    "\n",
    "# define seed; seeds tested (1452, 1994, 42, 7, 13800)\n",
    "SEED = 1994\n",
    "\n",
    "# define scenario\n",
    "# Note: Due to the nature of the utilized algorithms (User-User neighborhood methods), \n",
    "# only scenarios that include the 'validation in' set in the 'validation training' set, \n",
    "# and the 'test in' set in the 'full training' set, are applicable.\n",
    "scenario = WeakGeneralization(validation=True, seed=SEED)\n",
    "scenario.split(im)\n",
    "\n",
    "# define time threshold\n",
    "SECONDS = 24*3600\n",
    "\n",
    "# define number of evaluations\n",
    "EVALUATIONS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### UPDATE\n",
    "print(\"Users {} | Items: {}\".format(scenario.full_training_data.binary_values.shape[0], scenario.full_training_data.binary_values.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_objective(model, fit_args={}):\n",
    "    model.fit(scenario.validation_training_data.binary_values, **fit_args)\n",
    "\n",
    "    # generate predictions and mask training interactions\n",
    "    predictions = model.predict(scenario.validation_training_data.binary_values).toarray()\n",
    "    predictions[scenario.validation_training_data.binary_values.nonzero()] = -np.inf\n",
    "\n",
    "    ndcg, _ = tndcg_at_n(predictions, scenario.validation_data_out.binary_values, K)\n",
    "\n",
    "    return 1-ndcg\n",
    "\n",
    "def combined_objective(model, fit_args={}):\n",
    "    model.fit(scenario.validation_training_data.binary_values, **fit_args)\n",
    "\n",
    "    if \"users_features\" in fit_args: #fda\n",
    "        predictions = model.model_.predict().toarray()\n",
    "    else:\n",
    "        predictions = model.predict(scenario.validation_training_data.binary_values).toarray()\n",
    "    predictions[scenario.validation_training_data.binary_values.nonzero()] = -np.inf\n",
    "\n",
    "    ndcg, _ = tndcg_at_n(predictions, scenario.validation_data_out.binary_values, K)\n",
    "    equity, _, _ = c_equity_at_n(predictions[inner_uids_f, :], predictions[inner_uids_m, :], inner_genre_dict, K)\n",
    "\n",
    "    return alpha * (1-ndcg) + (1 - alpha) * equity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fairmf\n",
    "sst_field = torch.zeros((im.num_active_users, im.num_active_items), dtype=torch.bool)\n",
    "sst_field[inner_uids_f, :] = True\n",
    "\n",
    "# for fda\n",
    "users_features = np.zeros(im.num_active_users); users_features[inner_uids_m] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize EASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optimize ease\n",
    "optimisation_results_ease = fmin(\n",
    "    fn=lambda param: accuracy_objective(myEASE(l2=param[\"l2\"], method=\"user\")),\n",
    "    space={\"l2\": hp.loguniform(\"l2\", np.log(1e0), np.log(1e4))},\n",
    "    algo=tpe.suggest,\n",
    "    timeout = SECONDS,\n",
    "    max_evals = EVALUATIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize BNSlim (spend a lot of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize bnslim\n",
    "optimisation_results_bnslim = fmin(\n",
    "    fn=lambda param: combined_objective(BNSLIM(knn=100, l1=param[\"l1\"], l2=param[\"l2\"], l3=param[\"l3\"], method=\"user\", seed=SEED), {\"inner_ids_npr\": inner_uids_m}),\n",
    "    space={\"l1\": hp.loguniform(\"l1\", np.log(1e-3), np.log(7)),\n",
    "           \"l2\": hp.loguniform(\"l2\", np.log(1e-3), np.log(7)),\n",
    "           \"l3\": hp.loguniform(\"l3\", np.log(1e1), np.log(1e4))\n",
    "           }, \n",
    "    algo=tpe.suggest,\n",
    "    timeout=SECONDS,\n",
    "    max_evals=EVALUATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize FSLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize fslr\n",
    "optimisation_results_fslr = fmin(\n",
    "    fn=lambda param: combined_objective(FSLR(l1=param[\"l1\"], l2=param[\"l2\"], method=\"user\"), {\"inner_ids_pr\": inner_uids_f, \"inner_ids_npr\": inner_uids_m}),\n",
    "    space={\"l1\": hp.loguniform(\"l1\", np.log(1e-3), np.log(1e1)),\n",
    "           \"l2\": hp.loguniform(\"l2\", np.log(1e0), np.log(1e4))},\n",
    "    algo=tpe.suggest,\n",
    "    timeout=SECONDS,\n",
    "    max_evals=EVALUATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize BNSlim ADMM (proposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optimize bnslim admm\n",
    "optimisation_results_bnslim_admm = fmin(\n",
    "    fn=lambda param: combined_objective(BNSLIM_ADMM(l1=param[\"l1\"], l2=param[\"l2\"], l3=param[\"l3\"], method=\"user\"), {\"inner_ids_npr\": inner_uids_m}),\n",
    "    space={\"l1\": hp.loguniform(\"l1\", np.log(1e-3), np.log(50)),\n",
    "           \"l2\": hp.loguniform(\"l2\", np.log(1e0), np.log(1e4)),\n",
    "           \"l3\": hp.loguniform(\"l3\", np.log(1e-3), np.log(1e3))},\n",
    "    algo=tpe.suggest,\n",
    "    timeout = SECONDS,\n",
    "    max_evals = EVALUATIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize FairMF (proposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optimize FairMF\n",
    "factor_choices = [32, 64, 128]\n",
    "optimisation_results_fairmf = fmin(\n",
    "    fn=lambda param: combined_objective(FairMF(batch_size=im.num_active_users, learning_rate=param[\"learning_rate\"], l2=param[\"l2\"], num_factors=param[\"num_factors\"], seed=SEED), {\"sst_field\": sst_field}),\n",
    "    space={\"learning_rate\": hp.loguniform(\"learning_rate\", np.log(1e-6), np.log(1e0)),\n",
    "           \"l2\": hp.loguniform(\"l2\", np.log(1e-6), np.log(1e-1)),\n",
    "           \"num_factors\": hp.choice(\"num_factors\", factor_choices)\n",
    "           },\n",
    "    algo=tpe.suggest,\n",
    "    timeout=SECONDS,\n",
    "    max_evals=EVALUATIONS\n",
    ")\n",
    "\n",
    "optimisation_results_fairmf[\"num_factors\"] = factor_choices[optimisation_results_fairmf[\"num_factors\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize FDA (BPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optimize FDA\n",
    "# define the parameter choices\n",
    "num_ng_choices = [5, 7, 9, 10]\n",
    "ratio_choices = [0.1, 0.3, 0.5, 0.7]\n",
    "all_combinations = itertools.product(num_ng_choices, ratio_choices)\n",
    "\n",
    "best_params = None\n",
    "best_score = float(\"inf\")\n",
    "\n",
    "for num_ng, noise_ratio in all_combinations:\n",
    "    score = combined_objective(\n",
    "        FDA_bpr(num_ng=num_ng, noise_ratio=noise_ratio, seed=SEED), \n",
    "        {\"users_features\": users_features}\n",
    "    )\n",
    "\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_params = {\"num_ng\": num_ng, \"noise_ratio\": noise_ratio}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params = {}\n",
    "opt_params.update({\n",
    "    \"ease\": optimisation_results_ease,\n",
    "    \"bnslim\": optimisation_results_bnslim,\n",
    "    \"fslr\": optimisation_results_fslr,\n",
    "    \"bnslim_admm\": optimisation_results_bnslim_admm,\n",
    "    \"fairmf\": optimisation_results_fairmf,\n",
    "    \"fda\": best_params\n",
    "})\n",
    "\n",
    "folder = f\"ml-1m/{SEED}\"; os.makedirs(folder, exist_ok=True)\n",
    "with open(folder + \"/opt_params.json\", \"w\") as f: json.dump(opt_params, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gpt request method (for reranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "def ask_to_gpt(messages, gpt_model='gpt-4.1-nano',max_tokens=1024,temperature=0,n_retry=2,verbose=False):\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key = os.getenv('OPENAI_API_KEY')\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"================ OpenAI API call =================\")\n",
    "        print(\" > Model: \", gpt_model)\n",
    "        print(\" > temperature: \", temperature)\n",
    "        print(\" > max_tokens: \", max_tokens)\n",
    "        print(\"==================== Request =====================\")\n",
    "        #print(json.dumps(messages, indent=2))\n",
    "\n",
    "    while n_retry:\n",
    "        try:\n",
    "            if verbose:\n",
    "                print('Please wait...')\n",
    "            response = client.chat.completions.create(\n",
    "                  model=gpt_model,\n",
    "                  messages=messages,\n",
    "                  max_completion_tokens=max_tokens,\n",
    "                  #modalities=['text'],\n",
    "                  temperature=temperature,# [0-2]\n",
    "                  top_p=1,                \n",
    "                  frequency_penalty=0,    # [-2, 2]\n",
    "                  presence_penalty=0,     # [-2,2]\n",
    "                  n = 1,                  # [>1]\n",
    "              )\n",
    "            if verbose:\n",
    "                print(\"==================== Answer ======================\")\n",
    "                #display(response.choices[0].message.content)\n",
    "                print(\"==================== Stats =======================\")\n",
    "                print(dict(response).get('usage'))\n",
    "                print(\"==================================================\")\n",
    "\n",
    "            chat_completion = {\n",
    "                \"prompt\": messages,\n",
    "                \"completion\": response,\n",
    "            }\n",
    "            return chat_completion\n",
    "        except Exception as e:\n",
    "            print(f\"An exception occurred in the GPT request: {str(e)}\")\n",
    "            if n_retry > 0:\n",
    "                print('Trying request GPT again in few seconds!\\nWait...')\n",
    "            else:\n",
    "                raise f\"The trying limit was reached.\\nExiting... with the error: {str(e)}\"\n",
    "            \n",
    "            n_retry -= 1\n",
    "            time.sleep(5) # Wait 5 seconds\n",
    "            print('Trying again!')\n",
    "    raise \"The trying limit was reached.\\nExiting...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate data from original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_history(df_pp,inner_user_id):\n",
    "    raw_user_id = get_raw_user_ids(df_pp, [inner_user_id])[0]\n",
    "    user_history_items_title = ratings_merged.loc[ratings_merged.User_id == raw_user_id,['Title','Timestamp']].sort_values(by='Timestamp')['Title'].unique()\n",
    "    return [ original_to_updated_titles[uhit] for uhit in user_history_items_title ]\n",
    "\n",
    "def get_raw_item_id(title : str):\n",
    "    return ratings_merged.loc[ratings_merged.Title == title,'Item_id'].unique()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-step Reranking Method (Preferences + Recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preferences_prompt(user_history_items):\n",
    "  \n",
    "  persona = 'You are a movie recommender specialist, tell me what are my preferences and explain them.'\n",
    "  prompt = '''The movies I have watched (watched movies): {}.\n",
    "\n",
    "What features are most important to me when selecting movies (Summarize my preferences briefly)?\n",
    "Answer:'''.format( ', '.join(user_history_items) )\n",
    "   \n",
    "  return [{\"role\":\"system\", \"content\": persona}, {\"role\":\"user\",\"content\": prompt}]\n",
    "\n",
    "def build_recommendation_prompt(user_history_items,candidate_set, preferences_completion, n_items=10):\n",
    "    \n",
    "    persona = f'You are a movie recommender specialist, based on my preferences, recommend {n_items} movies to me watch next and explain the recommendations'\n",
    "\n",
    "    prompt = f'''Candidate Set (candidate movies): {', '.join(candidate_set)}.\n",
    "\n",
    "Recommend {n_items} movies from the Candidate Set based on my preferences and similarity with the movies I have watched.\n",
    "Format: [position. recommended movie name :: recommendation reason] as follow:\n",
    "1. Recommended item 1:: recommendation reason 1.\n",
    "2. Recommended item 2:: recommendation reason 2.\n",
    "\n",
    "Answer:'''\n",
    "\n",
    "    prefs_prompt = build_preferences_prompt(user_history_items)\n",
    "\n",
    "    recsys_prompt = [\n",
    "        {\"role\": \"system\", \"content\": persona},\n",
    "        {\"role\": \"user\", \"content\": prefs_prompt[1]['content']},\n",
    "        {\"role\": \"assistant\", \"content\": preferences_completion },\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    return recsys_prompt\n",
    "\n",
    "def generate_prompts(model_name, K, result_list, reranking_top_K=10):\n",
    "    test_cases = list()\n",
    "    # For each user\n",
    "    for inner_user_id, top_items in enumerate(result_list[model_name][f\"top_{K}\"]):\n",
    "        user_history = get_user_history(ratings_pp,inner_user_id) \n",
    "\n",
    "        sorted_top_items = sorted(top_items, key=lambda item: item['position']) # Keep the same item positions from backbones\n",
    "        user_candidate_set = [item['item_title'] for item in sorted_top_items]\n",
    "\n",
    "        user_history_reranking = []\n",
    "        for history_item in user_history: \n",
    "            if history_item not in user_candidate_set: # Avoid candidate set items in the user's history prompt input\n",
    "                user_history_reranking.append(history_item)\n",
    "       \n",
    "        if len(user_history) != len(user_history_reranking): # Only reranking users with ground-truth in the candidate set\n",
    "            user_preferences_prompt = build_preferences_prompt(user_history_reranking)\n",
    "            user_recsys_prompt = build_recommendation_prompt(user_history_reranking,\n",
    "                                                             user_candidate_set, \n",
    "                                                             \"###PREF_COMPLETION###\", # To be replaced after 1st completion\n",
    "                                                             n_items=reranking_top_K)\n",
    "            \n",
    "            reranking_user_info = {\n",
    "                'inner_user_id': inner_user_id,\n",
    "                'backbone' : model_name,\n",
    "                'candidates' : K,\n",
    "                'reranking_k' : reranking_top_K,\n",
    "                'preferences_prompt': user_preferences_prompt,\n",
    "                'recsys_prompt': user_recsys_prompt\n",
    "            }\n",
    "            test_cases.append(reranking_user_info) ## Only add user as test case for reranking if above conditions are met\n",
    "    return test_cases\n",
    "\n",
    "def get_2step_user_recommendations(user,gpt_model='gpt-4.1-nano',max_tokens=1024,temperature=0):\n",
    "    ## Step 1. Prompt Preferences\n",
    "    response_1 = ask_to_gpt(user['preferences_prompt'],gpt_model,max_tokens,temperature)\n",
    "    user['recsys_prompt'][2]['content'] = response_1['completion'].choices[0].message.content\n",
    "    ## Step 2. Prompt Recommendations\n",
    "    response_2 = ask_to_gpt(user['recsys_prompt'],gpt_model,max_tokens,temperature)\n",
    "\n",
    "    user['preferences_completion'] = response_1['completion'].choices[0].message.content\n",
    "    user['recsys_completion'] = response_2['completion'].choices[0].message.content\n",
    "    user['gpt_info'] = {\n",
    "        \"model\" : gpt_model,\n",
    "        \"max_tokens\" : max_tokens,\n",
    "        \"temperature\" : temperature\n",
    "    }\n",
    "    with open(f\"ml-1m/{SEED}/users_completions/UserCompletion-{user['inner_user_id']}.json\",\"w\") as f: json.dump(user,f,indent=2)\n",
    "    return user['preferences_completion'], user['recsys_completion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def format_recommendation_response(recsys_response : str) -> List[str]: # maybe it is need from typing import List and use List[str] (first List letter in Uppercase)\n",
    "    \"\"\"\n",
    "    Generate a list of recommendation based on the completion/response of\n",
    "    GPT API with each fund name followed by an explanation about the recommender.\n",
    "\n",
    "    Param:\n",
    "    @recsys_response: The completion/response generated by OpenAI API to the recommender prompt.\n",
    "\n",
    "    Return a list of recommended items with a little description about the recommendation. \n",
    "    The format of each item string is: [no. **name**:: description]. The ** are Markdown-based.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split response in items\n",
    "    split_pattern = '\\n\\s*\\n*'\n",
    "    rec_responses = re.split(split_pattern, recsys_response)\n",
    "\n",
    "    rec_items = []\n",
    "    for response in rec_responses:\n",
    "        if re.match(r\"^[0-9]\",response):\n",
    "            response = re.sub('\\*\\*','',response)\n",
    "            response = re.sub('\\*\\*','',response)\n",
    "            response = re.sub('(^[0-9]+)\\s*(–|-|\\.)\\s*','\\\\1. ',response)\n",
    "            response = re.sub('(^[0-9]+\\.)\\s*(–|-)','\\\\1',response)\n",
    "            response = re.sub('\\s(–|-)',':',response)\n",
    "            response = re.sub('(^[0-9]+)\\s*\\)','\\\\1.',response)\n",
    "            response = re.sub('(^[0-9]+\\.\\s)','\\\\1**',response)\n",
    "            response = re.sub('::','**::',response)\n",
    "            rec_items.append(response)\n",
    "            \n",
    "    return rec_items\n",
    "\n",
    "\n",
    "def split_recommendation_reason(recommendations : List[str]) -> List[Tuple[str,str]]:\n",
    "\n",
    "    result = []\n",
    "    for rec in recommendations:\n",
    "        parts = rec.split('::')\n",
    "        result.append( (parts[0].strip(),parts[1].strip()) )\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_rec_title_list(recommendations):\n",
    "    titles = split_recommendation_reason(format_recommendation_response(recommendations))\n",
    "    return [title.split(\"**\")[1] for title, reason in titles ]\n",
    "    \n",
    "\n",
    "def get_rec_pos_title(recommendations,pos):\n",
    "\n",
    "    title = split_recommendation_reason(format_recommendation_response(recommendations))\n",
    "    if len(title) < pos: # Número de recomendações menor do que a posição solicitada\n",
    "        raise \"Fewer recommended items than requested!\"\n",
    "    title = title[pos-1][0]\n",
    "    title = title.split(\"**\")[1]\n",
    "    title = title.replace('\"','')\n",
    "    title = title.replace(\"'\",'')\n",
    "    \n",
    "    return title.strip()\n",
    "\n",
    "\n",
    "def get_rec_pos_explanation(recommendations,pos):\n",
    "\n",
    "    title = split_recommendation_reason(format_recommendation_response(recommendations))\n",
    "    if len(title) < pos: # Número de recomendações menor do que a posição solicitada\n",
    "        raise \"Fewer recommended items than requested!\"\n",
    "    title = title[pos-1][1]\n",
    "    \n",
    "    return title.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### UPDATE\n",
    "def get_user_ranking(user_id,items_ids):\n",
    "    raw_items_ids = get_raw_item_ids(ratings_pp,items_ids)      # RAW: from dataset\n",
    "    #inner_items_ids = get_inner_item_ids(ratings_pp,items_ids) # INNER: used by recpack\n",
    "    rec_items = []\n",
    "    for i, item_id in enumerate(raw_items_ids):\n",
    "        item_original_title = movies_items[movies_items.Item_id == item_id].Title.values[0]\n",
    "        rec_item = {\n",
    "            \"item_id\": int(item_id),\n",
    "            \"item_title\": original_to_updated_titles[item_original_title],\n",
    "            \"position\": i+1\n",
    "        }\n",
    "        rec_items.append(rec_item)\n",
    "\n",
    "    return rec_items\n",
    "\n",
    "def get_all_users_ranking(top_n_predictions):\n",
    "    users = []\n",
    "    for inner_user_id, item_list in enumerate(top_n_predictions):\n",
    "        users.append(get_user_ranking(inner_user_id,item_list))\n",
    "    return users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, test and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"ml-1m/{SEED}/opt_params.json\", \"r\") as f: opt_params = json.load(f)\n",
    "\n",
    "def initialize_models(opt_params):\n",
    "    return {\n",
    "        \"ease\": myEASE(l2=opt_params[\"ease\"][\"l2\"], method=\"user\"),\n",
    "        \"bnslim\": BNSLIM(knn=100, l1=opt_params[\"bnslim\"][\"l1\"], l2=opt_params[\"bnslim\"][\"l2\"], l3=opt_params[\"bnslim\"][\"l3\"], maxIter=50, method=\"user\", seed=SEED),\n",
    "        \"fslr\": FSLR(l1=opt_params[\"fslr\"][\"l1\"], l2=opt_params[\"fslr\"][\"l2\"], method=\"user\"),\n",
    "        \"bnslim_admm\": BNSLIM_ADMM(l1=opt_params[\"bnslim_admm\"][\"l1\"], l2=opt_params[\"bnslim_admm\"][\"l2\"], l3=opt_params[\"bnslim_admm\"][\"l3\"], method=\"user\"),\n",
    "        \"fairmf\": FairMF(batch_size=im.num_active_users, l2=opt_params[\"fairmf\"][\"l2\"], learning_rate=opt_params[\"fairmf\"][\"learning_rate\"], num_factors=opt_params[\"fairmf\"][\"num_factors\"], seed=SEED),\n",
    "        \"fda\": FDA_bpr(\n",
    "            noise_ratio=opt_params[\"fda\"][\"noise_ratio\"], \n",
    "            num_ng=opt_params[\"fda\"][\"num_ng\"],\n",
    "            seed=SEED\n",
    "        )\n",
    "    }\n",
    "\n",
    "# initialize models\n",
    "models = initialize_models(opt_params)\n",
    "\n",
    "# define the models, list sizes, and metrics\n",
    "#list_sizes = [10, 20, 50, 100]\n",
    "list_sizes = [10,20]\n",
    "metrics = [\"recall\",'recall_reranking',\"ndcg\", \"ndcg_reranking\", ## UPDATE\n",
    "           \"c-equity\",\"c-equity_reranking\", \"u-parity\",\"u-parity_reranking\"] ## UPDATE\n",
    "\n",
    "# initialize a dictionary to store results with mean and standard deviation\n",
    "results = {\n",
    "    \"iters_num\": {model: 0 for model in [\"bnslim\", \"fslr\", \"bnslim_admm\", \"fairmf\"]},\n",
    "    \"fit_time\": {model: 0 for model in models.keys()},\n",
    "    \"reranking_sample_users\": {model: 0 for model in models.keys()}, ## UPDATE\n",
    "    **{metric: {model: {size: {\"mean\": 0, \"std\": 0} for size in list_sizes} for model in models.keys()} for metric in metrics},\n",
    "}\n",
    "\n",
    "result_list = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UPDATE\n",
    "reranking_test_cases = dict()\n",
    "reranking_list_size = 10   # Reranking to 10 items with LLM\n",
    "reranking_users_sample = 20 # 20 usuários por método para amostrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### Model info: ease - myEASE ########\n",
      "2025-09-04 04:18:32,809 - base - recpack - INFO - Fitting myEASE complete - Took 5.67s\n",
      "Inner User ID  3060 Reranked items (inner ids):  [ 20  61 236 260 267 269 271 294 379 457]\n",
      "Inner User ID  2753 Reranked items (inner ids):  [  2  12  13  18 130 133 136 137 621 906]\n",
      "Inner User ID  351 Reranked items (inner ids):  [ 12  20  23 129 221 261 337 461 464 467]\n",
      "Inner User ID  3050 Reranked items (inner ids):  [ 13 115 193 260 315 330 333 569 583 967]\n",
      "Inner User ID  5041 Reranked items (inner ids):  [   0   15  112  113  115  134  137  155  193 1410]\n",
      "Original title `Like Water for Chocolate` not recognized - User: 111 in the [ease,top_20_10]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 237 in the [ease,top_20_10]\n",
      "Inner User ID  705 Reranked items (inner ids):  [  19   40  155  261  330  457  514  639 1401 1929]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 3503 in the [ease,top_20_10]\n",
      "Original title `South Park: Bigger, Longer & Uncut` not recognized - User: 4519 in the [ease,top_20_10]\n",
      "Inner User ID  4715 Reranked items (inner ids):  [  2  15  37  40 356 385 413 433 521 781]\n",
      "Original title `The Empire Strikes Back` not recognized - User: 4648 in the [ease,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 4612 in the [ease,top_20_10]\n",
      "Inner User ID  1462 Reranked items (inner ids):  [ 18  61  81 133 184 219 305 324 402 577]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 4971 in the [ease,top_20_10]\n",
      "Inner User ID  5926 Reranked items (inner ids):  [ 210  271  408  560  569  702  709  716 1169 1382]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 4901 in the [ease,top_20_10]\n",
      "Inner User ID  4470 Reranked items (inner ids):  [  11   55   58   92   93   97   99  574  908 1113]\n",
      "Inner User ID  4371 Reranked items (inner ids):  [ 61  74 130 167 273 305 413 569 570 900]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 1542 in the [ease,top_20_10]\n",
      ">>> Users with hallucination in reranking: 9 / 20\n",
      "####### Model info: bnslim - BNSLIM ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eas/Documents/fair_neighborhood/src/recommenders/slim_bn.py:129: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  X_ik = np.sum(X[i,:] - W[i,mask] @ X[mask,:]) + self.l3 * p[k] * np.sum(p[mask] * W[i,mask])\n",
      "/Users/eas/Documents/fair_neighborhood/src/recommenders/slim_bn.py:129: RuntimeWarning: invalid value encountered in matmul\n",
      "  X_ik = np.sum(X[i,:] - W[i,mask] @ X[mask,:]) + self.l3 * p[k] * np.sum(p[mask] * W[i,mask])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 1715 in the [bnslim,top_20_10]\n",
      "Inner User ID  4354 Reranked items (inner ids):  [ 12  19  61 155 171 337 457 536 903 952]\n",
      "Inner User ID  2858 Reranked items (inner ids):  [ 11  12  37  61 141 152 193 497 564 636]\n",
      "Original title `Seven Samurai (The Magnificent Seven)` not recognized - User: 3680 in the [bnslim,top_20_10]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 3865 in the [bnslim,top_20_10]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 5069 in the [bnslim,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 1644 in the [bnslim,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 2641 in the [bnslim,top_20_10]\n",
      "Inner User ID  5272 Reranked items (inner ids):  [ 19  27 221 238 271 413 457 569 585 657]\n",
      "Inner User ID  393 Reranked items (inner ids):  [ 15  19  27  70 136 139 159 193 330 335]\n",
      "Inner User ID  1529 Reranked items (inner ids):  [  0  68 133 221 236 261 456 457 494 723]\n",
      "Inner User ID  4629 Reranked items (inner ids):  [  11   13   15  127  137  330  556  644  725 1313]\n",
      "Inner User ID  1869 Reranked items (inner ids):  [ 15  20  27 155 169 186 327 639 906 947]\n",
      "Original title `Star Wars: Episode VI: Return of the Jedi` not recognized - User: 1886 in the [bnslim,top_20_10]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 765 in the [bnslim,top_20_10]\n",
      "Inner User ID  1321 Reranked items (inner ids):  [ 11  15 100 133 236 261 408 413 569 585]\n",
      "Inner User ID  585 Reranked items (inner ids):  [  0  19  99 115 130 137 141 142 188 564]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 3421 in the [bnslim,top_20_10]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 3001 in the [bnslim,top_20_10]\n",
      "Original title `South Park: Bigger, Longer & Uncut` not recognized - User: 3754 in the [bnslim,top_20_10]\n",
      ">>> Users with hallucination in reranking: 11 / 20\n",
      "####### Model info: fslr - FSLR ########\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 5239 in the [fslr,top_20_10]\n",
      "Inner User ID  5660 Reranked items (inner ids):  [  2  12  13  15  19  27  37  86 139 193]\n",
      "Inner User ID  195 Reranked items (inner ids):  [  0  61  70 221 236 271 294 330 443 719]\n",
      "Inner User ID  3142 Reranked items (inner ids):  [  0  11  12  15  19  20  27 193 221 261]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 5440 in the [fslr,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 427 in the [fslr,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 5144 in the [fslr,top_20_10]\n",
      "Inner User ID  581 Reranked items (inner ids):  [  2  12 137 221 260 271 330 337 413 514]\n",
      "Inner User ID  1009 Reranked items (inner ids):  [ 12  13  15  37 127 139 221 236 238 267]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 3893 in the [fslr,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 5515 in the [fslr,top_20_10]\n",
      "Inner User ID  462 Reranked items (inner ids):  [  0  19  23  27  40 171 221 236 261 335]\n",
      "Inner User ID  2062 Reranked items (inner ids):  [ 23  27  61  90 261 267 330 333 413 639]\n",
      "Original title `Star Wars: Episode VI: Return of the Jedi` not recognized - User: 3406 in the [fslr,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 1057 in the [fslr,top_20_10]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 5126 in the [fslr,top_20_10]\n",
      "Inner User ID  5574 Reranked items (inner ids):  [  23  267  269  379  413  457  585  599  719 1929]\n",
      "Inner User ID  412 Reranked items (inner ids):  [ 13  19  86 130 137 139 221 236 273 413]\n",
      "Original title `Dr. Strangelove` not recognized - User: 5041 in the [fslr,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 5943 in the [fslr,top_20_10]\n",
      ">>> Users with hallucination in reranking: 11 / 20\n",
      "####### Model info: bnslim_admm - BNSLIM_ADMM ########\n",
      "Inner User ID  3841 Reranked items (inner ids):  [ 12  15  19  20  23  61  70 193 236 569]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 5502 in the [bnslim_admm,top_20_10]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 3698 in the [bnslim_admm,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 1393 in the [bnslim_admm,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 3962 in the [bnslim_admm,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 849 in the [bnslim_admm,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 5366 in the [bnslim_admm,top_20_10]\n",
      "Inner User ID  2611 Reranked items (inner ids):  [  0  12  13  15  19  20  27 136 141 221]\n",
      "Inner User ID  3928 Reranked items (inner ids):  [  23   60   90  115  133  137  141  305  315 1410]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 1954 in the [bnslim_admm,top_20_10]\n",
      "Inner User ID  5932 Reranked items (inner ids):  [  2  10  19  20 221 236 330 408 461 560]\n",
      "Original title `Star Wars: Episode VI: Return of the Jedi` not recognized - User: 5029 in the [bnslim_admm,top_20_10]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 214 in the [bnslim_admm,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 1858 in the [bnslim_admm,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 4617 in the [bnslim_admm,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 4663 in the [bnslim_admm,top_20_10]\n",
      "Original title `Star Wars: Episode VI: Return of the Jedi` not recognized - User: 3086 in the [bnslim_admm,top_20_10]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 4886 in the [bnslim_admm,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 4053 in the [bnslim_admm,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 5601 in the [bnslim_admm,top_20_10]\n",
      ">>> Users with hallucination in reranking: 16 / 20\n",
      "####### Model info: fairmf - <src.recommenders.mf_fair.FairMF object at 0x32012dbe0> ########\n",
      "Epoch 1/250, Loss: 0.0226\n",
      "Epoch 2/250, Loss: 0.0224\n",
      "Epoch 3/250, Loss: 0.0222\n",
      "Epoch 4/250, Loss: 0.0221\n",
      "Epoch 5/250, Loss: 0.0221\n",
      "Epoch 6/250, Loss: 0.0220\n",
      "Epoch 7/250, Loss: 0.0220\n",
      "Epoch 8/250, Loss: 0.0220\n",
      "Epoch 9/250, Loss: 0.0220\n",
      "Epoch 10/250, Loss: 0.0220\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 3645 in the [fairmf,top_20_10]\n",
      "Inner User ID  1890 Reranked items (inner ids):  [  0  12  19  61  70 141 271 413 583 585]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 243 in the [fairmf,top_20_10]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 411 in the [fairmf,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 4873 in the [fairmf,top_20_10]\n",
      "Inner User ID  2281 Reranked items (inner ids):  [  0   2  15  18  20  23  70 221 261 344]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 819 in the [fairmf,top_20_10]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 4455 in the [fairmf,top_20_10]\n",
      "Inner User ID  4265 Reranked items (inner ids):  [  0  12  23  61  88 129 141 193 221 261]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 3110 in the [fairmf,top_20_10]\n",
      "Inner User ID  367 Reranked items (inner ids):  [  0  15  23  37 112 221 236 305 408 413]\n",
      "Original title `Star Wars: Episode VI: Return of the Jedi` not recognized - User: 45 in the [fairmf,top_20_10]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 3950 in the [fairmf,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 1440 in the [fairmf,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 3053 in the [fairmf,top_20_10]\n",
      "Inner User ID  1117 Reranked items (inner ids):  [  2  69  88 141 155 193 221 271 330 443]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 5126 in the [fairmf,top_20_10]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 1790 in the [fairmf,top_20_10]\n",
      "Original title `Star Wars: Episode V: The Empire Strikes Back` not recognized - User: 5312 in the [fairmf,top_20_10]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 2295 in the [fairmf,top_20_10]\n",
      ">>> Users with hallucination in reranking: 15 / 20\n",
      "####### Model info: fda - <src.recommenders.fda.FDA_bpr object at 0x32012df40> ########\n",
      "Negative sampling, start\n",
      "Negative sampling, end\n",
      "epoch:1 time:38.3 loss task:0.692\n",
      "epoch:2 time:62.7 loss task:0.667\n",
      "epoch:3 time:87.1 loss task:0.591\n",
      "epoch:4 time:116.2 loss task:0.502\n",
      "epoch:5 time:141.1 loss task:0.453\n",
      "epoch:6 time:165.7 loss task:0.441\n",
      "epoch:7 time:190.3 loss task:0.444\n",
      "epoch:8 time:214.3 loss task:0.45\n",
      "epoch:9 time:238.6 loss task:0.456\n",
      "epoch:10 time:262.7 loss task:0.461\n",
      "epoch:11 time:285.1 loss task:0.465\n",
      "epoch:12 time:307.9 loss task:0.468\n",
      "epoch:13 time:332.1 loss task:0.471\n",
      "epoch:14 time:356.5 loss task:0.473\n",
      "epoch:15 time:381.2 loss task:0.474\n",
      "epoch:16 time:404.6 loss task:0.474\n",
      "epoch:17 time:428.0 loss task:0.474\n",
      "epoch:18 time:452.1 loss task:0.474\n",
      "epoch:19 time:475.4 loss task:0.473\n",
      "epoch:20 time:499.7 loss task:0.473\n",
      "epoch:21 time:523.4 loss task:0.473\n",
      "epoch:22 time:546.4 loss task:0.472\n",
      "epoch:23 time:570.3 loss task:0.472\n",
      "epoch:24 time:594.1 loss task:0.472\n",
      "epoch:25 time:618.3 loss task:0.471\n",
      "epoch:26 time:642.7 loss task:0.471\n",
      "epoch:27 time:667.0 loss task:0.47\n",
      "epoch:28 time:690.8 loss task:0.47\n",
      "epoch:29 time:715.1 loss task:0.47\n",
      "epoch:30 time:739.1 loss task:0.47\n",
      "epoch:31 time:764.0 loss task:0.469\n",
      "epoch:32 time:788.1 loss task:0.469\n",
      "epoch:33 time:812.9 loss task:0.469\n",
      "epoch:34 time:836.2 loss task:0.468\n",
      "epoch:35 time:859.6 loss task:0.467\n",
      "epoch:36 time:883.1 loss task:0.467\n",
      "epoch:37 time:906.3 loss task:0.465\n",
      "epoch:38 time:929.0 loss task:0.463\n",
      "epoch:39 time:953.8 loss task:0.461\n",
      "epoch:40 time:978.3 loss task:0.459\n",
      "epoch:41 time:1002.8 loss task:0.457\n",
      "epoch:42 time:1027.4 loss task:0.454\n",
      "epoch:43 time:1051.6 loss task:0.452\n",
      "epoch:44 time:1076.3 loss task:0.449\n",
      "epoch:45 time:1100.4 loss task:0.446\n",
      "epoch:46 time:1124.2 loss task:0.443\n",
      "epoch:47 time:1148.5 loss task:0.441\n",
      "epoch:48 time:1172.7 loss task:0.438\n",
      "epoch:49 time:1196.9 loss task:0.435\n",
      "epoch:50 time:1221.9 loss task:0.432\n",
      "epoch:51 time:1246.1 loss task:0.429\n",
      "epoch:52 time:1270.9 loss task:0.426\n",
      "epoch:53 time:1295.7 loss task:0.423\n",
      "epoch:54 time:1319.6 loss task:0.42\n",
      "epoch:55 time:1344.5 loss task:0.417\n",
      "epoch:56 time:1369.1 loss task:0.415\n",
      "epoch:57 time:1393.3 loss task:0.413\n",
      "epoch:58 time:1418.2 loss task:0.41\n",
      "epoch:59 time:1442.4 loss task:0.409\n",
      "epoch:60 time:1467.4 loss task:0.407\n",
      "epoch:61 time:1491.9 loss task:0.405\n",
      "epoch:62 time:1516.1 loss task:0.404\n",
      "epoch:63 time:1540.6 loss task:0.402\n",
      "epoch:64 time:1565.3 loss task:0.401\n",
      "epoch:65 time:1589.6 loss task:0.4\n",
      "epoch:66 time:1614.4 loss task:0.4\n",
      "epoch:67 time:1638.8 loss task:0.399\n",
      "epoch:68 time:1663.7 loss task:0.398\n",
      "epoch:69 time:1688.4 loss task:0.398\n",
      "epoch:70 time:1713.2 loss task:0.397\n",
      "epoch:71 time:1738.1 loss task:0.397\n",
      "epoch:72 time:1762.8 loss task:0.397\n",
      "epoch:73 time:1786.9 loss task:0.397\n",
      "epoch:74 time:1811.5 loss task:0.397\n",
      "epoch:75 time:1835.8 loss task:0.397\n",
      "epoch:76 time:1860.6 loss task:0.396\n",
      "epoch:77 time:1885.6 loss task:0.396\n",
      "epoch:78 time:1909.7 loss task:0.396\n",
      "epoch:79 time:1934.5 loss task:0.396\n",
      "epoch:80 time:1958.8 loss task:0.396\n",
      "epoch:81 time:1982.8 loss task:0.396\n",
      "epoch:82 time:2007.8 loss task:0.396\n",
      "epoch:83 time:2032.1 loss task:0.395\n",
      "epoch:84 time:2056.1 loss task:0.395\n",
      "epoch:85 time:2080.5 loss task:0.395\n",
      "epoch:86 time:2104.1 loss task:0.395\n",
      "epoch:87 time:2128.1 loss task:0.395\n",
      "epoch:88 time:2151.8 loss task:0.394\n",
      "epoch:89 time:2175.7 loss task:0.394\n",
      "epoch:90 time:2198.9 loss task:0.394\n",
      "epoch:91 time:2224.3 loss task:0.394\n",
      "epoch:92 time:2248.1 loss task:0.393\n",
      "epoch:93 time:2272.5 loss task:0.393\n",
      "epoch:94 time:2296.2 loss task:0.393\n",
      "epoch:95 time:2320.5 loss task:0.393\n",
      "epoch:96 time:2344.1 loss task:0.392\n",
      "epoch:97 time:2367.5 loss task:0.392\n",
      "epoch:98 time:2390.9 loss task:0.392\n",
      "epoch:99 time:2413.7 loss task:0.391\n",
      "epoch:100 time:2436.5 loss task:0.391\n",
      "epoch:101 time:2459.2 loss task:0.391\n",
      "epoch:102 time:2482.3 loss task:0.391\n",
      "epoch:103 time:2506.0 loss task:0.39\n",
      "epoch:104 time:2529.9 loss task:0.39\n",
      "epoch:105 time:2553.7 loss task:0.39\n",
      "epoch:106 time:2577.7 loss task:0.39\n",
      "epoch:107 time:2601.7 loss task:0.389\n",
      "epoch:108 time:2625.5 loss task:0.389\n",
      "epoch:109 time:2649.5 loss task:0.389\n",
      "epoch:110 time:2673.9 loss task:0.389\n",
      "epoch:111 time:2698.0 loss task:0.389\n",
      "epoch:112 time:2722.4 loss task:0.388\n",
      "epoch:113 time:2745.8 loss task:0.388\n",
      "epoch:114 time:2769.7 loss task:0.388\n",
      "epoch:115 time:2793.9 loss task:0.387\n",
      "epoch:116 time:2816.9 loss task:0.387\n",
      "epoch:117 time:2840.9 loss task:0.387\n",
      "epoch:118 time:2865.0 loss task:0.387\n",
      "epoch:119 time:2888.7 loss task:0.387\n",
      "epoch:120 time:2913.4 loss task:0.386\n",
      "epoch:121 time:2937.0 loss task:0.386\n",
      "epoch:122 time:2961.5 loss task:0.386\n",
      "epoch:123 time:2986.2 loss task:0.386\n",
      "epoch:124 time:3010.1 loss task:0.386\n",
      "epoch:125 time:3034.3 loss task:0.385\n",
      "epoch:126 time:3058.1 loss task:0.385\n",
      "epoch:127 time:3081.5 loss task:0.385\n",
      "epoch:128 time:3106.0 loss task:0.385\n",
      "epoch:129 time:3129.6 loss task:0.385\n",
      "epoch:130 time:3153.9 loss task:0.385\n",
      "epoch:131 time:3178.5 loss task:0.384\n",
      "epoch:132 time:3202.6 loss task:0.384\n",
      "epoch:133 time:3227.4 loss task:0.384\n",
      "epoch:134 time:3251.7 loss task:0.384\n",
      "epoch:135 time:3275.9 loss task:0.384\n",
      "Inner User ID  2743 Reranked items (inner ids):  [  23  330  461  569  709  735 1167 1169 2930 2979]\n",
      "Inner User ID  4972 Reranked items (inner ids):  [  41  236  267  290  315  335  472  496  968 1410]\n",
      "Inner User ID  979 Reranked items (inner ids):  [  85  582  664  685  692  937  975 1138 1528 1658]\n",
      "Inner User ID  2278 Reranked items (inner ids):  [ 57 130 294 325 344 409 418 456 457 581]\n",
      "Inner User ID  5482 Reranked items (inner ids):  [  11   50   54  184  315  325  413  569  903 1181]\n",
      "Inner User ID  1767 Reranked items (inner ids):  [  88   89  299  487  518  720  725  819  949 1116]\n",
      "Inner User ID  93 Reranked items (inner ids):  [ 11  82  93  96  99 170 478 504 571 574]\n",
      "Inner User ID  5046 Reranked items (inner ids):  [267 283 333 456 457 514 530 592 608 719]\n",
      "Original title `Star Wars: Episode VI: Return of the Jedi` not recognized - User: 5399 in the [fda,top_20_10]\n",
      "Inner User ID  4504 Reranked items (inner ids):  [  83  151  324  502  547  621 1062 2363 2524 2807]\n",
      "Inner User ID  985 Reranked items (inner ids):  [ 15  19 152 342 345 359 413 458 687 702]\n",
      "Inner User ID  4293 Reranked items (inner ids):  [  3  12  70  89 137 141 171 188 458 787]\n",
      "Inner User ID  4406 Reranked items (inner ids):  [   2   37   60  137  152  193  209  318  385 3268]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 3606 in the [fda,top_20_10]\n",
      "Inner User ID  2996 Reranked items (inner ids):  [ 36  44 193 238 349 422 526 795 831 901]\n",
      "Original title `Star Wars: Episode IV: A New Hope` not recognized - User: 2207 in the [fda,top_20_10]\n",
      "Inner User ID  602 Reranked items (inner ids):  [ 50 124 135 152 170 337 342 392 801]\n",
      "Inner User ID  4539 Reranked items (inner ids):  [ 15 133 139 141 142 152 305 595 931 945]\n",
      "Inner User ID  1331 Reranked items (inner ids):  [ 194  204  455  488  569  745 1014 1109 1591 1743]\n",
      "Original title `Battleship Potemkin` not recognized - User: 2752 in the [fda,top_20_10]\n",
      ">>> Users with hallucination in reranking: 4 / 20\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    params = {}\n",
    "    if model_name == \"fslr\":\n",
    "        params = {\"inner_ids_pr\": inner_uids_f, \"inner_ids_npr\": inner_uids_m}\n",
    "    elif model_name in [\"bnslim\", \"bnslim_admm\"]:\n",
    "        params = {\"inner_ids_npr\": inner_uids_m}\n",
    "    elif model_name == \"fairmf\":\n",
    "        params = {\"sst_field\": sst_field}\n",
    "    elif model_name == \"fda\":\n",
    "        params = {\"users_features\": users_features}\n",
    "\n",
    "    print(f\"####### Model info: {model_name} - {model} ########\")\n",
    "    ## TRAINING\n",
    "    start_time = time.time()\n",
    "    model.fit(scenario.full_training_data.binary_values, **params)\n",
    "    results[\"fit_time\"][model_name] = time.time() - start_time\n",
    "\n",
    "    if model_name in results[\"iters_num\"]:\n",
    "        if model_name == \"fairmf\":\n",
    "            results[\"iters_num\"][model_name] = model.epochs\n",
    "        else:\n",
    "            results[\"iters_num\"][model_name] = model.iters\n",
    "\n",
    "    ## TEST\n",
    "    # generate predictions and mask training interactions\n",
    "    if model_name == \"fda\":\n",
    "        y_pred = model.model_.predict()\n",
    "    else:\n",
    "        y_pred = model.predict(scenario.full_training_data.binary_values)\n",
    "\n",
    "    predictions = y_pred.toarray()\n",
    "    predictions[scenario.full_training_data.binary_values.nonzero()] = -np.inf\n",
    "\n",
    "    ## EVALUATE\n",
    "    # compute evaluation metrics for different values of K\n",
    "    result_list[model_name] = {}\n",
    "    reranking_test_cases[model_name] = dict()\n",
    "    for K in list_sizes:\n",
    "        #### UPDATE ####\n",
    "        ## Get candidate set for reranking from original backbones (for each model and top K)\n",
    "        result_list[f\"{model_name}\"][f\"top_{K}\"] = get_all_users_ranking(get_topn_indices(predictions,K))\n",
    "\n",
    "#############################\n",
    "        if K > reranking_list_size: # Only reranks if backbone filtering has more items than reranking list size\n",
    "            ## Reranking STARTs here\n",
    "            reranking_test_cases[model_name][f'top_{K}'] = generate_prompts(model_name, K, \n",
    "                                                                        result_list,\n",
    "                                                                        reranking_top_K=reranking_list_size)\n",
    "\n",
    "            sample_users = random.sample(reranking_test_cases[model_name][f'top_{K}'], reranking_users_sample)\n",
    "            \n",
    "            hallucination = 0 # Count users with any wrong item title identification problem (find exactly the same original name in dataset)\n",
    "            reranked_predictions = predictions.copy()\n",
    "            for user in sample_users:\n",
    "                #print(user)\n",
    "                ## Process LLM reranking\n",
    "                \n",
    "                ###############\n",
    "                ### LLM CALL --->>> Costs here\n",
    "                user_prefs, user_recs = get_2step_user_recommendations(user)\n",
    "                ###############\n",
    "                \n",
    "                ## Retrieve raw list IDs from LLM reranked item titles\n",
    "                original_title_list = []\n",
    "                item_title_problem = False # When LLMs generates a item title different than available ones in dataset/candidate set\n",
    "                for title in get_rec_title_list(user_recs):\n",
    "                    ### Title string response Post-processing (source of some hallucinations \"exactly match\" with catalog title string)\n",
    "                    title = title.strip()\n",
    "                    title = title.replace(\"’\",\"'\")\n",
    "                    ### \n",
    "                    if title in updated_to_original_titles:\n",
    "                        original_title_list.append(updated_to_original_titles[title])\n",
    "                    else:\n",
    "                        print(f\"Original title `{title}` not recognized - User: {user['inner_user_id']} in the [{model_name},top_{K}_{reranking_list_size}]\")\n",
    "                        item_title_problem = True\n",
    "                        break\n",
    "    \n",
    "                if not item_title_problem:\n",
    "                    result_list_id = [get_raw_item_id(original_title) for original_title in original_title_list ]\n",
    "                    ## Get inner item IDs\n",
    "                    result_inner_ids = get_inner_item_ids(ratings_pp,result_list_id)\n",
    "    \n",
    "                    ## Evaluate result\n",
    "                    print('Inner User ID ',user['inner_user_id'],'Reranked items (inner ids): ', result_inner_ids)\n",
    "                    # Update predictions based on reranking result\n",
    "                    user_max_pred = max(predictions[user['inner_user_id']])\n",
    "                    for i,result_item_index in enumerate(result_inner_ids):\n",
    "                        reranked_predictions[user['inner_user_id']][result_item_index] = user_max_pred+len(result_inner_ids)-i\n",
    "                else:\n",
    "                    hallucination += 1\n",
    "\n",
    "            print(f\">>> Users with hallucination in reranking: {hallucination} / {len(sample_users)}\")\n",
    "\n",
    "            results[\"reranking_sample_users\"][model_name] = len(sample_users) - hallucination\n",
    "            # new accuracy metrics\n",
    "            results[\"ndcg_reranking\"][model_name][reranking_list_size][\"mean\"], results[\"ndcg_reranking\"][model_name][reranking_list_size][\"std\"] = tndcg_at_n(reranked_predictions, scenario.test_data_out.binary_values, reranking_list_size)\n",
    "            results[\"recall_reranking\"][model_name][reranking_list_size][\"mean\"], results[\"recall_reranking\"][model_name][reranking_list_size][\"std\"] = recall_at_n(reranked_predictions, scenario.test_data_out.binary_values, reranking_list_size)\n",
    "\n",
    "            # fairness metrics\n",
    "            results[\"c-equity_reranking\"][model_name][reranking_list_size][\"mean\"], results[\"c-equity\"][model_name][reranking_list_size][\"std\"], _ = c_equity_at_n(predictions[inner_uids_f, :], predictions[inner_uids_m, :], inner_genre_dict, reranking_list_size)\n",
    "\n",
    "            females = np.ones(im.num_active_users); females[inner_uids_m] = 0\n",
    "            results[\"u-parity_reranking\"][model_name][reranking_list_size][\"mean\"], results[\"u-parity\"][model_name][reranking_list_size][\"std\"] = u_parity_at_n(predictions, females, inner_genre_dict, reranking_list_size)\n",
    "            \n",
    "#############################\n",
    "\n",
    "        # accuracy metrics\n",
    "        results[\"ndcg\"][model_name][K][\"mean\"], results[\"ndcg\"][model_name][K][\"std\"] = tndcg_at_n(predictions, scenario.test_data_out.binary_values, K)\n",
    "        results[\"recall\"][model_name][K][\"mean\"], results[\"recall\"][model_name][K][\"std\"] = recall_at_n(predictions, scenario.test_data_out.binary_values, K)\n",
    "\n",
    "        # fairness metrics\n",
    "        results[\"c-equity\"][model_name][K][\"mean\"], results[\"c-equity\"][model_name][K][\"std\"], _ = c_equity_at_n(predictions[inner_uids_f, :], predictions[inner_uids_m, :], inner_genre_dict, K)\n",
    "\n",
    "        females = np.ones(im.num_active_users); females[inner_uids_m] = 0\n",
    "        results[\"u-parity\"][model_name][K][\"mean\"], results[\"u-parity\"][model_name][K][\"std\"] = u_parity_at_n(predictions, females, inner_genre_dict, K)\n",
    "\n",
    "    #break # Apenas 1 modelo\n",
    "    # # save model\n",
    "    #pickle.dump(model, open(f\"ml-1m/{SEED}/{model_name}.pkl\", \"wb\"))\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "with open(f\"ml-1m/{SEED}/results.json\", \"w\") as f: json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list results\n",
    "with open(f\"ml-1m/{SEED}/result_lists.json\",\"w\") as f: json.dump(result_list,f,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Reranking (after backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load users candidate set lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"ml-1m/{SEED}/result_lists.json\",\"r\") as f: result_list = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking as post-processing (test reranking method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reranking_test_cases = dict()\n",
    "list_sizes = [20] # Reranking from 20 items filtered by backbones\n",
    "reranking_list_size = 10 # Reranking to 10 items with LLM\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if model_name not in result_list:\n",
    "        print(f'Without results for the model: {model_name}')\n",
    "        continue\n",
    "\n",
    "    reranking_test_cases[model_name] = dict()\n",
    "    for K in list_sizes:\n",
    "        if f\"top_{K}\" not in result_list[model_name]:\n",
    "            print(f'Without results for the model {model_name} @ {K}')\n",
    "            continue\n",
    "\n",
    "        ## Reranking STARTs here\n",
    "        reranking_test_cases[model_name][f'top_{K}'] = generate_prompts(model_name, K, \n",
    "                                                                        result_list,\n",
    "                                                                        reranking_top_K=reranking_list_size)\n",
    "\n",
    "        hallucination = 0 # Count users with any wrong item title identification problem (find exactly the same original name in dataset)\n",
    "        for user in reranking_test_cases[model_name][f'top_{K}']:\n",
    "            print(user)\n",
    "            ## Process LLM reranking\n",
    "            user_prefs, user_recs = get_2step_user_recommendations(user)\n",
    "            \n",
    "            ## Retrieve raw list IDs from LLM reranked item titles\n",
    "\n",
    "            original_title_list = []\n",
    "            item_title_problem = False\n",
    "            for title in get_rec_title_list(user_recs):\n",
    "                try:\n",
    "                    original_title_list.append(updated_to_original_titles[title])\n",
    "                except:\n",
    "                    print(f\"Original title `{title}` not recognized - User: {user}\")\n",
    "                    item_title_problem = True\n",
    "                    break\n",
    "\n",
    "            if not item_title_problem:\n",
    "                result_list_id = [get_raw_item_id(original_title) for original_title in original_title_list ]\n",
    "                ## Get inner item IDs\n",
    "                result_inner_ids = get_inner_item_ids(ratings_pp,result_list_id)\n",
    "\n",
    "                ## Evaluate result\n",
    "                \n",
    "            else:\n",
    "                hallucination += 1\n",
    "            \n",
    "            #result_list_original_titles = [ updated_to_original_titles[title] for title in get_rec_title_list(user_recsys) ] # Not safe            \n",
    "            #result_list_id = [get_raw_item_id(original_title) for original_title in result_list_original_titles ]\n",
    "            ## Get inner item IDs\n",
    "            #result_inner_ids = get_inner_item_ids(ratings_pp,result_list_id)\n",
    "\n",
    "            break # Apenas 1 usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list_original_titles = [ updated_to_original_titles[title] for title in get_rec_title_list(user_recsys) ]\n",
    "result_list_id = [get_raw_item_id(original_title) for original_title in result_list_original_titles ]\n",
    "\n",
    "# Get recpack inner item IDs\n",
    "result_inner_ids = get_inner_item_ids(ratings_pp,result_list_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample_users = random.sample(reranking_test_cases['ease']['top_20'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for user in sample_users:\n",
    "    # Get reranking list from LLM (2-step method)\n",
    "    prefs, recs = get_2step_user_recommendations(user) # Default: Uses gpt-4-nano, max_tokens=1024, temperature=0\n",
    "    #prefs5, recs5 = get_2step_user_recommendations(user,gpt_model='gpt-5-nano',max_tokens=8192,temperature=1)\n",
    "    \n",
    "    # Retrieve raw list IDs from LLM reranked item titles\n",
    "    result_list_original_titles = [ updated_to_original_titles[title] for title in get_rec_title_list(recs) ]\n",
    "    result_list_id = [get_raw_item_id(original_title) for original_title in result_list_original_titles ]\n",
    "\n",
    "    # Get recpack inner item IDs\n",
    "    result_inner_ids = get_inner_item_ids(ratings_pp,result_list_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
